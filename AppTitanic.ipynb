{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e47d53a",
   "metadata": {},
   "source": [
    "<div style=\"display: table; width: 100%;\">\n",
    "  <div style=\"display: table-cell; text-align: center; vertical-align: middle; width: 70%;\">\n",
    "    <h1>Herramientas para Data Science</h1>\n",
    "  </div>\n",
    "  <div style=\"display: table-cell; text-align: center; vertical-align: middle; width: 30%;\">\n",
    "    <img src=\"https://github.com/UIDE-Tareas/6-Herramientas-Data-Science-Tarea1/blob/main/Assets/UideLogo.png?raw=true\" alt=\"logo UIDE\" style=\"width:50%;\">\n",
    "  </div>\n",
    "</div>\n",
    "<hr />\n",
    "\n",
    "### ðŸŸ¦ Componente PrÃ¡ctico 1  \n",
    "ðŸŸ¡ Grupo: 3      \n",
    "ðŸŸ¡ Semana: 1      \n",
    "ðŸŸ¡ Docente:  Ing. IvÃ¡n GarcÃ­a S., PhD. (idgs78@hotmail.com)     \n",
    "\n",
    "### ðŸŸ¦ Realizado por:   \n",
    "Estudiantes\n",
    "\n",
    "ðŸ’» Evelin Rosero OrdoÃ±ez   \n",
    "\n",
    "ðŸ’» Marjorie Muso Tandalla\n",
    "\n",
    "ðŸ’» JosÃ© Espinoza Bone\n",
    "\n",
    "### ðŸŸ¦ Objetivo y alcance del trabajo \n",
    "- Esta prÃ¡ctica tiene el objetivo de realizar un preprocesamiento de datos y un AnÃ¡lisis \n",
    "exploratorio de datos (estadÃ­sticas y visualizaciÃ³n) de las variables mÃ¡s relevantes \n",
    "que considere para el propÃ³sito de predicciÃ³n de supervivencia en el desastre del Titanic. Al finalizar la \n",
    "prÃ¡ctica, los maestrantes podrÃ¡n manipular adecuadamente un dataset de datos \n",
    "estructurados (alfanumÃ©ricos). [Titanic - Machine Learning from Disaster](https://www.kaggle.com/competitions/titanic)\n",
    "\n",
    "\n",
    "### ðŸŸ¦ [CÃ³digo fuente original](https://github.com/UIDE-Tareas/6-Herramientas-Data-Science-Tarea1.git)\n",
    "Con [git](https://git-scm.com/) instalado. En Windows, Linux o MacOS ejecutar el comando.\n",
    "\n",
    "```\n",
    "git clone \"https://github.com/UIDE-Tareas/6-Herramientas-Data-Science-Tarea1.git\"\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914bb6d5",
   "metadata": {},
   "source": [
    "# 0ï¸âƒ£ Preparar entorno\n",
    "\n",
    "Funciones base para utilizar si son requeridas en el presente notebook. Adicional hay funciones utilitarias para utilizar con pandas.DataFrame y finalmente las funciones para cumplir con los objetivos del presente trabajo prÃ¡ctico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d374159d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILIDADES PARA GESTIÃ“N DE DEPENDENCIAS E INFORMACIÃ“N DEL ENTORNO\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "import os\n",
    "from pathlib import Path\n",
    "from enum import Enum\n",
    "import zipfile\n",
    "from typing import Optional\n",
    "from typing import Iterable\n",
    "from dataclasses import dataclass\n",
    "from typing import cast\n",
    "from typing import Tuple\n",
    "from types import SimpleNamespace\n",
    "from typing import Any\n",
    "from typing import Protocol\n",
    "from typing import Literal\n",
    "from typing import Sequence\n",
    "\n",
    "# Libs a instalar\n",
    "LIBS = [\n",
    "    \"numpy\",\n",
    "    \"pandas\",\n",
    "    \"matplotlib\",\n",
    "    \"seaborn\",\n",
    "    \"scikit-learn\",\n",
    "    \"requests\",\n",
    "    \"wcwidth\",\n",
    "]\n",
    "\n",
    "class ConsoleColor(Enum):\n",
    "    RED = \"\\033[91m\"\n",
    "    GREEN = \"\\033[92m\"\n",
    "    YELLOW = \"\\033[93m\"\n",
    "    BLUE = \"\\033[94m\"\n",
    "    MAGENTA = \"\\033[95m\"\n",
    "    CYAN = \"\\033[96m\"\n",
    "    WHITE = \"\\033[97m\"\n",
    "    RESET = \"\\033[0m\"\n",
    "\n",
    "\n",
    "def PrintColor(message: str, color: ConsoleColor) -> str:\n",
    "    RESET = ConsoleColor.RESET.value\n",
    "    return f\"{color.value}{message}{RESET}\"\n",
    "\n",
    "\n",
    "def ShowMessage(\n",
    "    message: str, title: str, icon: str, color: ConsoleColor, end: str = \"\\n\"\n",
    "):\n",
    "    colored_title = PrintColor(icon + f\"  \" + title.upper() + \":\", color)\n",
    "    print(f\"{colored_title} {message}\", end=end)\n",
    "\n",
    "\n",
    "def ShowInfoMessage(\n",
    "    message: str, title: str = \"Info\", icon: str = \"â„¹ï¸\", end: str = \"\\n\"\n",
    "):\n",
    "    ShowMessage(message, title, icon, ConsoleColor.CYAN, end)\n",
    "\n",
    "\n",
    "def ShowSuccessMessage(\n",
    "    message: str, title: str = \"Success\", icon: str = \"âœ…\", end: str = \"\\n\"\n",
    "):\n",
    "    ShowMessage(message, title, icon, ConsoleColor.GREEN, end)\n",
    "\n",
    "\n",
    "def ShowErrorMessage(\n",
    "    message: str, title: str = \"Error\", icon: str = \"âŒ\", end: str = \"\\n\"\n",
    "):\n",
    "    ShowMessage(message, title, icon, ConsoleColor.RED, end)\n",
    "\n",
    "\n",
    "def ShowWarningMessage(\n",
    "    message: str, title: str = \"Warning\", icon: str = \"âš ï¸\", end: str = \"\\n\"\n",
    "):\n",
    "    ShowMessage(message, title, icon, ConsoleColor.YELLOW, end)\n",
    "\n",
    "\n",
    "# Funcion para ejecutar comandos\n",
    "def RunCommand(\n",
    "    commandList: list[str], printCommand: bool = True, printError: bool = True\n",
    ") -> subprocess.CompletedProcess[str]:\n",
    "    print(\"â³\", \" \".join(commandList))\n",
    "\n",
    "    if printCommand:\n",
    "        proc = subprocess.Popen(\n",
    "            commandList,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True,\n",
    "            bufsize=1,\n",
    "            universal_newlines=True,\n",
    "        )\n",
    "\n",
    "        out_lines: list[str] = []\n",
    "        assert proc.stdout is not None\n",
    "        for line in proc.stdout:\n",
    "            print(line, end=\"\")\n",
    "            out_lines.append(line)\n",
    "\n",
    "        proc.wait()\n",
    "        err_text = \"\"\n",
    "        if proc.stderr is not None:\n",
    "            err_text = proc.stderr.read() or \"\"\n",
    "\n",
    "        if proc.returncode != 0 and printError and err_text:\n",
    "            ShowErrorMessage(err_text, \"\", end=\"\")\n",
    "            # print(err_text, end=\"\")\n",
    "\n",
    "        return subprocess.CompletedProcess(\n",
    "            args=commandList,\n",
    "            returncode=proc.returncode,\n",
    "            stdout=\"\".join(out_lines),\n",
    "            stderr=err_text,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        result = subprocess.run(\n",
    "            commandList, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True\n",
    "        )\n",
    "        if result.returncode != 0 and printError and result.stderr:\n",
    "            ShowErrorMessage(result.stderr, \"\", end=\"\")\n",
    "            # print(result.stderr, end=\"\")\n",
    "        return result\n",
    "\n",
    "\n",
    "# FunciÃ³n para instalar las dependencias\n",
    "def InstallDeps(libs: Optional[list[str]] = None):\n",
    "    print(\"â„¹ï¸ Installing deps.\")\n",
    "    printCommand = False\n",
    "    printError = True\n",
    "    RunCommand(\n",
    "        [sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"],\n",
    "        printCommand=printCommand,\n",
    "        printError=printError,\n",
    "    )\n",
    "    if libs is None or libs.count == 0:\n",
    "        print(\"No hay elementos a instalar.\")\n",
    "    else:\n",
    "        RunCommand(\n",
    "            [sys.executable, \"-m\", \"pip\", \"install\", *libs],\n",
    "            printCommand=printCommand,\n",
    "            printError=printError,\n",
    "        )\n",
    "        print(\"Deps installed.\")\n",
    "    print()\n",
    "\n",
    "\n",
    "# FunciÃ³n para mostrar info el ambiente de ejecuciÃ³n\n",
    "def ShowEnvironmentInfo():\n",
    "    print(\"â„¹ï¸  Environment Info:\")\n",
    "    print(\"Python Version:\", sys.version)\n",
    "    print(\"Platform:\", sys.platform)\n",
    "    print(\"Executable Path:\", sys.executable)\n",
    "    print(\"Current Working Directory:\", os.getcwd())\n",
    "    print(\"VIRTUAL_ENV:\", os.environ.get(\"VIRTUAL_ENV\"))\n",
    "    print(\"sys.prefix:\", sys.prefix)\n",
    "    print(\"sys.base_prefix:\", sys.base_prefix)\n",
    "    print()\n",
    "\n",
    "\n",
    "InstallDeps(LIBS)\n",
    "ShowEnvironmentInfo()\n",
    "import requests\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class BoxStyle:\n",
    "    TL: str\n",
    "    TR: str\n",
    "    BL: str\n",
    "    BR: str\n",
    "    H: str\n",
    "    V: str\n",
    "\n",
    "class TitleBoxLineStyle(Enum):\n",
    "    SIMPLE = BoxStyle(\"â”Œ\", \"â”\", \"â””\", \"â”˜\", \"â”€\", \"â”‚\")\n",
    "    DOUBLE = BoxStyle(\"â•”\", \"â•—\", \"â•š\", \"â•\", \"â•\", \"â•‘\")\n",
    "    ROUNDED = BoxStyle(\"â•­\", \"â•®\", \"â•°\", \"â•¯\", \"â”€\", \"â”‚\")\n",
    "    HEAVY = BoxStyle(\"â”\", \"â”“\", \"â”—\", \"â”›\", \"â”\", \"â”ƒ\")\n",
    "    ASCII = BoxStyle(\"+\", \"+\", \"+\", \"+\", \"-\", \"|\")\n",
    "    DOUBLE_BOLD = BoxStyle(\"â•”\", \"â•—\", \"â•š\", \"â•\", \"â•¬\", \"â•‘\")\n",
    "    BLOCK = BoxStyle(\"â–ˆ\", \"â–ˆ\", \"â–ˆ\", \"â–ˆ\", \"â–ˆ\", \"â–ˆ\")\n",
    "    HEAVY_CROSS = BoxStyle(\"â•’\", \"â••\", \"â•˜\", \"â•›\", \"â•ª\", \"â”ƒ\")\n",
    "    METAL = BoxStyle(\"â•ž\", \"â•¡\", \"â•˜\", \"â•›\", \"â•\", \"â•‘\")\n",
    "\n",
    "\n",
    "# FunciÃ³n para mostrar un tÃ­tulo con recuadro\n",
    "def ShowTitleBox(\n",
    "    text: str,\n",
    "    max_len: int = 100,\n",
    "    boxLineStyle: TitleBoxLineStyle = TitleBoxLineStyle.SIMPLE,\n",
    "    color: ConsoleColor = ConsoleColor.CYAN,\n",
    "):\n",
    "    try:\n",
    "\n",
    "        def vislen(s: str) -> int:\n",
    "            from wcwidth import wcswidth as _w\n",
    "\n",
    "            n = _w(s)\n",
    "            return n if n >= 0 else len(s)\n",
    "\n",
    "    except Exception:\n",
    "\n",
    "        def vislen(s: str) -> int:\n",
    "            return len(s)\n",
    "\n",
    "    pad = 1\n",
    "    tlen = vislen(text)\n",
    "    inner = max(max_len, tlen)\n",
    "    left = (inner - tlen) // 2\n",
    "    right = inner - tlen - left\n",
    "\n",
    "    top = f\"{boxLineStyle.value.TL}{boxLineStyle.value.H * (inner + 2 * pad)}{boxLineStyle.value.TR}\"\n",
    "    mid = f\"{boxLineStyle.value.V}{' ' * pad}{' ' * left}{text}{' ' * right}{' ' * pad}{boxLineStyle.value.V}\"\n",
    "    bot = f\"{boxLineStyle.value.BL}{boxLineStyle.value.H * (inner + 2 * pad)}{boxLineStyle.value.BR}\"\n",
    "    print(PrintColor(\"\\n\".join([top, mid, bot]), color))\n",
    "\n",
    "\n",
    "# FunciÃ³n para descargar un archivo\n",
    "def DownloadFile(uri: str, filename: str, overwrite: bool = False, timeout: int = 20, printInfo: bool = True):\n",
    "    dest = Path(filename).resolve()\n",
    "    if dest.exists() and dest.is_file() and dest.stat().st_size > 0 and not overwrite:\n",
    "        if printInfo:\n",
    "            print(\n",
    "                f'âœ… Ya existe: \"{dest}\". No se descarga (use overwrite=True para forzar).'\n",
    "            )\n",
    "        return\n",
    "    if dest.parent and not dest.parent.exists():\n",
    "        dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if printInfo:\n",
    "        print(f'â„¹ï¸ Descargando \"{uri}\" â†’ \"{dest}\"')\n",
    "    try:\n",
    "        with requests.get(uri, stream=True, timeout=timeout) as resp:\n",
    "            resp.raise_for_status()\n",
    "            tmp = dest.with_suffix(dest.suffix + \".part\")\n",
    "            with open(tmp, \"wb\") as f:\n",
    "                for chunk in resp.iter_content(chunk_size=1024 * 64):\n",
    "                    if chunk:  # filtra keep-alive chunks\n",
    "                        f.write(chunk)\n",
    "            tmp.replace(dest)\n",
    "        if printInfo: \n",
    "            print(f'âœ… Archivo \"{dest}\" descargado exitosamente.')\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"âŒ Error al descargar: {e}\")\n",
    "\n",
    "\n",
    "# FunciÃ³n para descomprimir un archivo zip\n",
    "def UnzipFile(filename: str, outputDir: str):\n",
    "    print(f'â„¹ï¸ Descomprimiendo \"{filename}\" en \"{outputDir}\"')\n",
    "    try:\n",
    "        with zipfile.ZipFile(filename, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(outputDir)\n",
    "        print(f\"Descomprimido en: {os.path.abspath(outputDir)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413cec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILIDADES PARA ANÃLISIS Y MANIPULACIÃ“N DE DATAFRAMES\n",
    "\n",
    "# Importar libraries\n",
    "import pandas as pd\n",
    "import pandas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.axes import Axes\n",
    "\n",
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from sklearn.utils import Bunch\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, classification_report,\n",
    "    precision_score, recall_score, f1_score\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configurar opciones de Pandas\n",
    "pd.set_option(\"display.float_format\", \"{:.2f}\".format)\n",
    "pandas.set_option(\"display.max_rows\", None)\n",
    "pandas.set_option(\"display.max_columns\", None)\n",
    "\n",
    "\n",
    "# FunciÃ³n para mostrar la informaciÃ³n del DataFrame.\n",
    "def ShowDfInfo(df: pandas.DataFrame, title):\n",
    "    display(f\"â„¹ï¸ INFO {title} â„¹ï¸\")\n",
    "    df.info()\n",
    "    display()\n",
    "\n",
    "\n",
    "# FunciÃ³n para mostrar las n primeras filas del DataFrame.\n",
    "def ShowDfHead(df: pandas.DataFrame, title: str, headQty=10):\n",
    "    display(f\"â„¹ï¸ {title}: Primeros {headQty} elementos.\")\n",
    "    display(df.head(headQty))\n",
    "    display()\n",
    "\n",
    "\n",
    "# FunciÃ³n para mostrar las n Ãºltimas filas del DataFrame.\n",
    "def ShowDfTail(df: pandas.DataFrame, title: str, tailQty=10):\n",
    "    display(f\"â„¹ï¸ {title}: Ãšltimos {tailQty} elementos.\")\n",
    "    display(df.tail(tailQty))\n",
    "    display()\n",
    "\n",
    "\n",
    "# Mostrar el tamaÃ±o del DataFrame\n",
    "def ShowDfShape(df: pandas.DataFrame, title: str):\n",
    "    display(f\"â„¹ï¸ {title} - TamaÃ±o de los datos\")\n",
    "    display(f\"{df.shape[0]} filas x {df.shape[1]} columnas\")\n",
    "    display()\n",
    "\n",
    "\n",
    "# FunciÃ³n para mostrar la estadÃ­stica descriptiva de todas las columnas del DataFrame, por tipo de dato.\n",
    "def ShowDfStats(df: pandas.DataFrame, title: str = \"\"):\n",
    "    display(f\"â„¹ï¸ EstadÃ­stica descriptiva - {title}\")\n",
    "    numeric_cols = df.select_dtypes(include=\"number\")\n",
    "    if not numeric_cols.empty:\n",
    "        display(\"    ðŸ”¢ Columnas numÃ©ricas\".upper())\n",
    "        numeric_desc = (\n",
    "            numeric_cols.describe().round(2).T\n",
    "        )  # Transpuesta para aÃ±adir columna\n",
    "        numeric_desc[\"var\"] = numeric_cols.var(numeric_only=True).round(2)\n",
    "        display(numeric_desc.T)\n",
    "    non_numeric_cols = df.select_dtypes(\n",
    "        include=[\"boolean\", \"string\", \"category\", \"object\"]\n",
    "    )\n",
    "    if not non_numeric_cols.empty:\n",
    "        display(\"    ðŸ”¡ Columnas no numÃ©ricas\".upper())\n",
    "        non_numeric_desc = non_numeric_cols.describe()\n",
    "        display(non_numeric_desc)\n",
    "    datetime_cols = df.select_dtypes(include=[\"datetime\", \"datetimetz\"])\n",
    "    if not datetime_cols.empty:\n",
    "        display(\"    ðŸ“… Columnas fechas\".upper())\n",
    "        datetime_desc = datetime_cols.describe()\n",
    "        display(datetime_desc)\n",
    "\n",
    "\n",
    "# FunciÃ³n para mostrar una visiÃ³n general completa del DataFrame\n",
    "def ShowFullDfOverview(df, title, headQty=5, tailQty=5):\n",
    "    ShowDfInfo(df, title)\n",
    "    ShowDfStats(df, title)\n",
    "    ShowDfShape(df, title)\n",
    "    ShowDfHead(df, title, headQty=headQty)\n",
    "    ShowDfTail(df, title, tailQty=tailQty)\n",
    "\n",
    "\n",
    "# FunciÃ³n para mostrar los valores nulos o NaN de cada columna en un DataFrame\n",
    "def ShowDfNanValues(df: pandas.DataFrame, title: str):\n",
    "    display(f\"â„¹ï¸ Contador de valores Nulos - {title}\")\n",
    "    nulls_count = df.isnull().sum()\n",
    "    nulls_df = nulls_count.reset_index()\n",
    "    nulls_df.columns = [\"Columna\", \"Cantidad_Nulos\"]\n",
    "    display(nulls_df)\n",
    "    display()\n",
    "\n",
    "\n",
    "# Tipos de correlaciÃ³n\n",
    "class CorrelationType(Enum):\n",
    "    ALL = \"all\"\n",
    "    STRONG = \"strong\"\n",
    "    WEAK = \"weak\"\n",
    "\n",
    "\n",
    "# Muestra las correlaciones completas, dÃ©biles y fuertes.\n",
    "def ShowDfCorrelation(\n",
    "    df: pandas.DataFrame,\n",
    "    title: str,\n",
    "    fig: Optional[Figure] = None,\n",
    "    ax: Optional[Axes] = None,\n",
    "    level: CorrelationType = CorrelationType.ALL,\n",
    "    umbral: float = 0.6,\n",
    "    showTable: bool = False,\n",
    "    figsize: tuple = (8, 6),\n",
    "    annotate: bool = True,\n",
    "):\n",
    "    if fig is None or ax is None:\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    display(f\"â„¹ï¸ {(title).upper()} - Matriz de CorrelaciÃ³n, Type: {level.name}\")\n",
    "\n",
    "    corr = df.select_dtypes(include=[\"number\"]).corr().copy()\n",
    "\n",
    "    if level == CorrelationType.STRONG:\n",
    "        corr = corr.where(np.abs(corr) >= umbral)\n",
    "    elif level == CorrelationType.WEAK:\n",
    "        corr = corr.where(np.abs(corr) < umbral)\n",
    "        np.fill_diagonal(corr.values, 1)\n",
    "    elif level != CorrelationType.ALL:\n",
    "        raise ValueError(f\"Invalid level: {level}\")\n",
    "\n",
    "    cax = ax.matshow(corr, vmin=-1, vmax=1)\n",
    "\n",
    "    cols = corr.columns\n",
    "    ax.set_xticks(range(len(cols)))\n",
    "    ax.set_yticks(range(len(cols)))\n",
    "    ax.set_xticklabels(cols, rotation=90, ha=\"left\")\n",
    "    ax.set_yticklabels(cols)\n",
    "\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    if annotate:\n",
    "        for (i, j), value in np.ndenumerate(corr.values):\n",
    "            if not np.isnan(value):\n",
    "                ax.text(j, i, f\"{value:+.2f}\", ha=\"center\", va=\"center\")\n",
    "\n",
    "    if level == CorrelationType.ALL:\n",
    "        titulo = \"Matriz de correlaciÃ³n completa\"\n",
    "    else:\n",
    "        titulo = f\"Matriz de correlaciÃ³n ({level.name}, umbral={umbral})\"\n",
    "\n",
    "    total_elementos = corr.size\n",
    "    total_nodiagonal = corr.size - corr.shape[0]\n",
    "    total_nan = corr.isna().sum().sum()\n",
    "    total_validos = total_elementos - total_nan - corr.shape[0]\n",
    "\n",
    "    titulo = (\n",
    "        f\"{titulo}, Total Matriz: {total_nodiagonal}, \"\n",
    "        f\"Total vÃ¡lidos: {total_validos} ({((total_validos * 100) / total_nodiagonal):.2f}%)\"\n",
    "    )\n",
    "\n",
    "    ax.set_title(titulo, pad=20)\n",
    "    ax.grid(False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    if showTable:\n",
    "        display(corr)\n",
    "    fig.show()\n",
    "    return fig, corr\n",
    "\n",
    "\n",
    "def NormalizeColumnNames(df: pandas.DataFrame) -> pandas.DataFrame:\n",
    "    df.columns = [\n",
    "        col.strip().title().replace(\" \", \"\").replace(\"_\", \"\") for col in df.columns\n",
    "    ]\n",
    "    return df\n",
    "\n",
    "\n",
    "def DropColumns(\n",
    "    df: pandas.DataFrame, toDrop: list[str], inplace: bool = False\n",
    ") -> pandas.DataFrame:\n",
    "    if not toDrop:\n",
    "        return df\n",
    "    if inplace:\n",
    "        df.drop(columns=df.columns.intersection(toDrop), inplace=True)\n",
    "        return df\n",
    "    else:\n",
    "        return df.drop(columns=df.columns.intersection(toDrop))\n",
    "\n",
    "\n",
    "# Para almacenar los datos del dataset\n",
    "@dataclass\n",
    "class Dataset:\n",
    "    X: pandas.DataFrame\n",
    "    y: pandas.DataFrame\n",
    "\n",
    "\n",
    "# Para almacenar los datos de split del dataset.\n",
    "@dataclass\n",
    "class DatasetSplit:\n",
    "    Train: Dataset\n",
    "    Test: Dataset\n",
    "\n",
    "\n",
    "# Muestra el head de cada componente del split.\n",
    "def ShowDatasetSplitHead(split: DatasetSplit, title: str, headQty: int = 5):\n",
    "    ShowDfHead(split.Train.X, f\"{title} - X Train\", headQty)\n",
    "    ShowDfHead(split.Train.y, f\"{title} - y Train\", headQty)\n",
    "    ShowDfHead(split.Test.X, f\"{title} - X Test\", headQty)\n",
    "    ShowDfHead(split.Test.y, f\"{title} - y Test\", headQty)\n",
    "\n",
    "\n",
    "# Muestra la informaciÃ³n del Dataset\n",
    "def ShowDatasetInfo(data: Dataset, title):\n",
    "    tAux = title\n",
    "    title = f\"{tAux} - Caracteristicas - X\"\n",
    "    ShowDfInfo(data.X, title)\n",
    "    ShowDfShape(data.X, title)\n",
    "    ShowDfStats(data.X, title)\n",
    "    ShowDfNanValues(data.X, title)\n",
    "    ShowDfHead(data.X, title)\n",
    "    ShowDfTail(data.X, title)\n",
    "    title = f\"{tAux} - CaracterÃ­sticas - y\"\n",
    "    ShowDfInfo(data.y, title)\n",
    "    ShowDfShape(data.y, title)\n",
    "    ShowDfStats(data.y, title)\n",
    "    ShowDfNanValues(data.y, title)\n",
    "    ShowDfHead(data.y, title)\n",
    "    ShowDfTail(data.y, title)\n",
    "\n",
    "\n",
    "# Muestra la informaciÃ³n del Dataset Split\n",
    "def ShowDatasetSplitInfo(split: DatasetSplit, title: str, headQty: int = 5):\n",
    "    tAux = title\n",
    "    title = f\"{tAux} - TRAIN\"\n",
    "    ShowDatasetInfo(split.Train, title)\n",
    "    title = f\"{tAux} - TEST\"\n",
    "    ShowDatasetInfo(split.Test, title)\n",
    "\n",
    "\n",
    "# Realiza el split del Dataset, en Train y test utilizando el ratio.\n",
    "def SplitDataset(\n",
    "    data: Dataset, trainRatio: float = 0.8, randomState: int = 42\n",
    ") -> DatasetSplit:\n",
    "    y_strat = data.y.iloc[:, 0]\n",
    "    XTrain, XTest, yTrain, yTest = train_test_split(\n",
    "        data.X,\n",
    "        data.y,\n",
    "        train_size=trainRatio,\n",
    "        random_state=randomState,\n",
    "        stratify=y_strat,\n",
    "    )\n",
    "    return DatasetSplit(\n",
    "        Train=Dataset(X=XTrain.reset_index(drop=True), y=yTrain.reset_index(drop=True)),\n",
    "        Test=Dataset(X=XTest.reset_index(drop=True), y=yTest.reset_index(drop=True)),\n",
    "    )\n",
    "\n",
    "\n",
    "# Contrato para los escaladores\n",
    "class ScalerProtocol(Protocol):\n",
    "    def fit(self, X, y: Any = None) -> Any: ...\n",
    "    def transform(self, X) -> Any: ...\n",
    "    def fit_transform(self, X, y: Any = None) -> Any: ...\n",
    "\n",
    "\n",
    "# Para almacenar los datos del dataset aplicado el escalador.\n",
    "@dataclass\n",
    "class ScaledDatasetSplit(DatasetSplit):\n",
    "    Scaler: ScalerProtocol\n",
    "\n",
    "# Enum para los tipos de escaladores soportados\n",
    "class ScalerType(Enum):\n",
    "    STANDARD = \"Standard\"\n",
    "    MIN_MAX = \"minmax\"\n",
    "    ROBUST = \"robust\"\n",
    "    MAX_ABS = \"maxabs\"\n",
    "    NORMALIZER = \"normalizer\"\n",
    "    QUANTILE = \"quantile\"\n",
    "    POWER = \"power\"\n",
    "    FUNCTION = \"function\"\n",
    "\n",
    "\n",
    "# Crea una instancia de scaler segÃºn el Enum ScalerType.\n",
    "def CreateScaler(scalerType: ScalerType, **kwargs) -> ScalerProtocol:\n",
    "    if scalerType == ScalerType.STANDARD:\n",
    "        return StandardScaler(**kwargs)\n",
    "    if scalerType == ScalerType.MIN_MAX:\n",
    "        return MinMaxScaler(**kwargs)\n",
    "    if scalerType == ScalerType.ROBUST:\n",
    "        return RobustScaler(**kwargs)\n",
    "    if scalerType == ScalerType.MAX_ABS:\n",
    "        return MaxAbsScaler(**kwargs)\n",
    "    if scalerType == ScalerType.NORMALIZER:\n",
    "        return Normalizer(**kwargs)\n",
    "    if scalerType == ScalerType.QUANTILE:\n",
    "        return QuantileTransformer(**kwargs)\n",
    "    if scalerType == ScalerType.POWER:\n",
    "        return PowerTransformer(**kwargs)\n",
    "    if scalerType == ScalerType.FUNCTION:\n",
    "        return FunctionTransformer(**kwargs)\n",
    "    raise ValueError(f\"ScalerType no soportado: {scalerType}\")\n",
    "\n",
    "def DetectScaler(scaler: ScalerProtocol) -> ScalerType:\n",
    "    if isinstance(scaler, StandardScaler):\n",
    "        return ScalerType.STANDARD\n",
    "    if isinstance(scaler, MinMaxScaler):\n",
    "        return ScalerType.MIN_MAX\n",
    "    if isinstance(scaler, RobustScaler):\n",
    "        return ScalerType.ROBUST\n",
    "    if isinstance(scaler, MaxAbsScaler):\n",
    "        return ScalerType.MAX_ABS\n",
    "    if isinstance(scaler, Normalizer):\n",
    "        return ScalerType.NORMALIZER\n",
    "    if isinstance(scaler, QuantileTransformer):\n",
    "        return ScalerType.QUANTILE\n",
    "    if isinstance(scaler, PowerTransformer):\n",
    "        return ScalerType.POWER\n",
    "    if isinstance(scaler, FunctionTransformer):\n",
    "        return ScalerType.FUNCTION\n",
    "    raise ValueError(f\"No se reconoce el tipo de scaler: {type(scaler)}\")\n",
    "\n",
    "# Escala el split usando el escalador proporcionado y retorna el split escalado.\n",
    "def ScaleDatasetSplit(\n",
    "    split: DatasetSplit, scaler: ScalerProtocol = StandardScaler()\n",
    ") -> ScaledDatasetSplit:\n",
    "    XTrainScaledValues = scaler.fit_transform(split.Train.X)\n",
    "    XTestScaledValues = scaler.transform(split.Test.X)\n",
    "\n",
    "    XTrainScaled = pandas.DataFrame(\n",
    "        XTrainScaledValues, columns=split.Train.X.columns, index=split.Train.X.index\n",
    "    )\n",
    "\n",
    "    XTestScaled = pandas.DataFrame(\n",
    "        XTestScaledValues, columns=split.Test.X.columns, index=split.Test.X.index\n",
    "    )\n",
    "\n",
    "    TrainScaledDataset = Dataset(X=XTrainScaled, y=split.Train.y.copy())\n",
    "    TestScaledDataset = Dataset(X=XTestScaled, y=split.Test.y.copy())\n",
    "\n",
    "    return ScaledDatasetSplit(\n",
    "        Train=TrainScaledDataset, Test=TestScaledDataset, Scaler=scaler\n",
    "    )\n",
    "\n",
    "# Para almacenar los datos del dataset aplicado PCA.\n",
    "@dataclass\n",
    "class PcaDatasetSplit(DatasetSplit):\n",
    "    Pca: PCA\n",
    "    Scaler: ScalerProtocol | None = None \n",
    "\n",
    "# Aplica PCA al split escalado y retorna el split con PCA aplicado.\n",
    "def ApplyPCA(\n",
    "    split: ScaledDatasetSplit,\n",
    "    explainedVarianceRatioSum: float = 0.95,\n",
    "    randomState: int = 42\n",
    ") -> PcaDatasetSplit:\n",
    "\n",
    "    def GetPCNames(n: int) -> list[str]:\n",
    "        return [f\"PC{i}\" for i in range(1, n + 1)]\n",
    "\n",
    "    pca = PCA(n_components=explainedVarianceRatioSum, random_state=randomState)\n",
    "\n",
    "    XTrainPCA = pca.fit_transform(split.Train.X)\n",
    "    XTestPCA = pca.transform(split.Test.X)\n",
    "\n",
    "    XTrainPcaDf = pandas.DataFrame(\n",
    "        XTrainPCA, index=split.Train.X.index, columns=GetPCNames(XTrainPCA.shape[1])\n",
    "    )\n",
    "\n",
    "    XTestPcaDf = pandas.DataFrame(\n",
    "        XTestPCA, index=split.Test.X.index, columns=GetPCNames(XTestPCA.shape[1])\n",
    "    )\n",
    "\n",
    "    return PcaDatasetSplit(\n",
    "        Train=Dataset(X=XTrainPcaDf, y=split.Train.y.copy()),\n",
    "        Test=Dataset(X=XTestPcaDf, y=split.Test.y.copy()),\n",
    "        Pca=pca,\n",
    "        Scaler=split.Scaler\n",
    "    )\n",
    "\n",
    "\n",
    "# Para almacenar los resultados de la regresiÃ³n logÃ­stica\n",
    "@dataclass\n",
    "class LogisticRegressionResult:\n",
    "    Model: LogisticRegression\n",
    "    Predictions: pandas.DataFrame\n",
    "    Accuracy: float\n",
    "    Precision: float\n",
    "    Recall: float\n",
    "    F1: float\n",
    "    ConfusionMatrix: np.ndarray\n",
    "    Iters: list[int] | np.ndarray\n",
    "    Report: str\n",
    "    Target: str\n",
    "\n",
    "# Aplica regresiÃ³n logÃ­stica al split escalado y retorna el resultado.\n",
    "def ApplyLogisticRegression(\n",
    "    split: ScaledDatasetSplit,\n",
    "    targetColumn: str,\n",
    "    randomState: int = 42,\n",
    "    maxIter: int = 200,\n",
    "    C: float = 1.0,\n",
    "    penalty: Literal['l1', 'l2', 'elasticnet'] | None = \"l2\",\n",
    "    solver: Literal['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'] = \"lbfgs\"\n",
    ") -> LogisticRegressionResult:\n",
    "    \n",
    "    XTrain = split.Train.X\n",
    "    yTrain = split.Train.y[targetColumn]\n",
    "    XTest = split.Test.X\n",
    "    yTest = split.Test.y[targetColumn]\n",
    "\n",
    "    model = LogisticRegression(\n",
    "        max_iter=maxIter,\n",
    "        random_state=randomState,\n",
    "        C=C,\n",
    "        penalty=penalty,\n",
    "        solver=solver\n",
    "    )\n",
    "\n",
    "    model.fit(XTrain, yTrain)\n",
    "\n",
    "    yPredTrain = model.predict(XTrain)\n",
    "    yPredTest = model.predict(XTest)\n",
    "\n",
    "    yProbaTrain = model.predict_proba(XTrain)\n",
    "    yProbaTest = model.predict_proba(XTest)\n",
    "\n",
    "    dfProbaTest = pandas.DataFrame(\n",
    "        yProbaTest,\n",
    "        index=XTest.index,\n",
    "        columns=[f\"Class-{cls}-Prob\" for cls in model.classes_]\n",
    "    )\n",
    "\n",
    "    dfPredTest = pandas.DataFrame(\n",
    "        {\n",
    "            \"yReal\": yTest.values,\n",
    "            \"yPred\": yPredTest\n",
    "        },\n",
    "        index=XTest.index\n",
    "    )\n",
    "\n",
    "    dfPredTest = pandas.concat([dfPredTest, dfProbaTest], axis=1)\n",
    "\n",
    "    acc = accuracy_score(yTest, yPredTest)\n",
    "    prec = precision_score(yTest, yPredTest, average=\"weighted\", zero_division=0)\n",
    "    rec = recall_score(yTest, yPredTest, average=\"weighted\", zero_division=0)\n",
    "    f1 = f1_score(yTest, yPredTest, average=\"weighted\", zero_division=0)\n",
    "    cm = confusion_matrix(yTest, yPredTest)\n",
    "    report = classification_report(yTest, yPredTest)\n",
    "\n",
    "    return LogisticRegressionResult(\n",
    "        Model=model,\n",
    "        Predictions=dfPredTest,\n",
    "        Accuracy=float(acc),\n",
    "        ConfusionMatrix=cm,\n",
    "        Precision=float(prec),\n",
    "        Recall=float(rec),\n",
    "        F1=float(f1),\n",
    "        Iters=model.n_iter_,\n",
    "        Report=str(report),\n",
    "        Target=targetColumn\n",
    "    )\n",
    "\n",
    "# Para almacenar los resultados de KNN\n",
    "@dataclass\n",
    "class KnnResult:\n",
    "    Model: KNeighborsClassifier\n",
    "    Predictions: pandas.DataFrame\n",
    "    Accuracy: float\n",
    "    Precision: float\n",
    "    Recall: float\n",
    "    F1: float\n",
    "    ConfusionMatrix: np.ndarray\n",
    "    Report: str\n",
    "    Target: str\n",
    "    K: int\n",
    "\n",
    "def GetBestKForKNN(\n",
    "    split: ScaledDatasetSplit,\n",
    "    targetColumn: str,\n",
    "    kMax: int = 30\n",
    ") -> int:\n",
    "    \n",
    "    XTrain = split.Train.X\n",
    "    yTrain = split.Train.y[targetColumn]\n",
    "    XTest = split.Test.X\n",
    "    yTest = split.Test.y[targetColumn]\n",
    "\n",
    "    bestK = 1\n",
    "    bestScore = -1.0\n",
    "\n",
    "    for k in range(1, kMax + 1):\n",
    "        model = KNeighborsClassifier(n_neighbors=k)\n",
    "        model.fit(XTrain, yTrain)\n",
    "\n",
    "        yPred = model.predict(XTest)\n",
    "        acc = accuracy_score(yTest, yPred)\n",
    "\n",
    "        if acc > bestScore:\n",
    "            bestScore = acc\n",
    "            bestK = k\n",
    "    return bestK\n",
    "\n",
    "def ApplyKNN(\n",
    "    split: ScaledDatasetSplit,\n",
    "    targetColumn: str,\n",
    "    weights: Literal[\"uniform\", \"distance\"] = \"uniform\",\n",
    "    metric: Literal[\n",
    "        \"euclidean\", \"manhattan\", \"chebyshev\", \"minkowski\"\n",
    "    ] = \"minkowski\"\n",
    ") -> KnnResult:\n",
    "    K = GetBestKForKNN(split, targetColumn)\n",
    "    XTrain = split.Train.X\n",
    "    yTrain = split.Train.y[targetColumn]\n",
    "    XTest = split.Test.X\n",
    "    yTest = split.Test.y[targetColumn]\n",
    "\n",
    "    model = KNeighborsClassifier(\n",
    "        n_neighbors=K,\n",
    "        weights=weights,\n",
    "        metric=metric\n",
    "    )\n",
    "\n",
    "    model.fit(XTrain, yTrain)\n",
    "\n",
    "    yPred = model.predict(XTest)\n",
    "\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        yProba = model.predict_proba(XTest)\n",
    "        dfProba = pandas.DataFrame(\n",
    "            yProba,\n",
    "            index=XTest.index,\n",
    "            columns=[f\"Class-{cls}-Prob\" for cls in model.classes_],\n",
    "        )\n",
    "    else:\n",
    "        dfProba = pandas.DataFrame(index=XTest.index)\n",
    "\n",
    "    dfPred = pandas.DataFrame(\n",
    "        {\n",
    "            \"yReal\": yTest.values,\n",
    "            \"yPred\": yPred,\n",
    "        },\n",
    "        index=XTest.index\n",
    "    )\n",
    "\n",
    "    dfPred = pandas.concat([dfPred, dfProba], axis=1)\n",
    "\n",
    "    acc = accuracy_score(yTest, yPred)\n",
    "    prec = precision_score(yTest, yPred, average=\"weighted\", zero_division=0)\n",
    "    rec = recall_score(yTest, yPred, average=\"weighted\", zero_division=0)\n",
    "    f1 = f1_score(yTest, yPred, average=\"weighted\", zero_division=0)\n",
    "    cm = confusion_matrix(yTest, yPred)\n",
    "    report = classification_report(yTest, yPred)\n",
    "\n",
    "    return KnnResult(\n",
    "        Model=model,\n",
    "        Predictions=dfPred,\n",
    "        Accuracy=float(acc),\n",
    "        Precision=float(prec),\n",
    "        Recall=float(rec),\n",
    "        F1=float(f1),\n",
    "        ConfusionMatrix=cm,\n",
    "        Report=str(report),\n",
    "        Target=targetColumn,\n",
    "        K=K\n",
    "    )\n",
    "\n",
    "\n",
    "def PlotConfusionMatrix(\n",
    "    cm,\n",
    "    classNames: Sequence[str] | None = None,\n",
    "    title: str = \"Confusion Matrix\"\n",
    "):\n",
    "    # Si classNames es None â†’ usar \"auto\"\n",
    "    xticks = classNames if classNames is not None else \"auto\"\n",
    "    yticks = classNames if classNames is not None else \"auto\"\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=xticks,\n",
    "        yticklabels=yticks\n",
    "    )\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Utilidades para detecciÃ³n de tipos de split\n",
    "@dataclass(frozen=True)\n",
    "class SplitTypeInfo:\n",
    "    IsPCA: bool\n",
    "    IsScaled: bool\n",
    "    IsRaw: bool\n",
    "\n",
    "# Detecta el tipo de split (PCA, Escalado, Crudo)\n",
    "def DetectSplitType(split) -> SplitTypeInfo:\n",
    "    isPca = isinstance(split, PcaDatasetSplit)\n",
    "    isScaled = isinstance(split, ScaledDatasetSplit)\n",
    "    isRaw = not isPca and not isScaled\n",
    "\n",
    "    return SplitTypeInfo(\n",
    "        IsPCA=isPca,\n",
    "        IsScaled=isScaled,\n",
    "        IsRaw=isRaw\n",
    "    )\n",
    "\n",
    "def GenerateMetricsTable(splitList: list[DatasetSplit], targetColumn:str) -> pandas.DataFrame:\n",
    "    results = []\n",
    "\n",
    "    for idx, split in enumerate(splitList, start=1):\n",
    "\n",
    "        # Detectar tipo de split\n",
    "        splitInfo = DetectSplitType(split)\n",
    "\n",
    "        # Nombre del scaler o None\n",
    "        scalerName = (\n",
    "            DetectScaler(split.Scaler).value\n",
    "            if (splitInfo.IsScaled and hasattr(split, \"Scaler\") and split.Scaler is not None)\n",
    "            else \"None\"\n",
    "        )\n",
    "\n",
    "        # Tipo del split\n",
    "        if splitInfo.IsPCA:\n",
    "            splitType = \"PCA\"\n",
    "        elif splitInfo.IsScaled:\n",
    "            splitType = \"SCALED\"\n",
    "        else:\n",
    "            splitType = \"RAW\"\n",
    "\n",
    "        # Ejecutar modelos\n",
    "        resultLR = ApplyLogisticRegression(split, targetColumn=targetColumn)\n",
    "        resultKNN = ApplyKNN(split, targetColumn=targetColumn)\n",
    "\n",
    "        # Guardar info en lista\n",
    "        results.append({\n",
    "            \"Index\": idx,\n",
    "            \"Type\": splitType,\n",
    "            \"Scaler\": scalerName,\n",
    "            # MÃ©tricas Logistic Regression\n",
    "            \"LR_Accuracy\": resultLR.Accuracy,\n",
    "            \"LR_Precision\": resultLR.Precision,\n",
    "            \"LR_Recall\": resultLR.Recall,\n",
    "            \"LR_F1\": resultLR.F1,\n",
    "            # MÃ©tricas KNN\n",
    "            \"KNN_BestK\": resultKNN.K,\n",
    "            \"KNN_Accuracy\": resultKNN.Accuracy,\n",
    "            \"KNN_Precision\": resultKNN.Precision,\n",
    "            \"KNN_Recall\": resultKNN.Recall,\n",
    "            \"KNN_F1\": resultKNN.F1,\n",
    "        })\n",
    "\n",
    "    # Convertir a DataFrame final\n",
    "    df = pandas.DataFrame(results)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def EvaluateSplits(splitList: list[DatasetSplit], targetColumn: str):\n",
    "\n",
    "    i = 0\n",
    "    for split in splitList:\n",
    "        splitInfo = DetectSplitType(split)\n",
    "        i += 1\n",
    "\n",
    "        title = (\n",
    "            f\"{i}.- Data - Scaled = {splitInfo.IsScaled} - PCA = {splitInfo.IsPCA} \"\n",
    "            f\"- Features({split.Train.X.columns.size}): \"\n",
    "            + \", \".join(split.Train.X.columns.tolist())\n",
    "            + \", Scaler: \"\n",
    "            + (\n",
    "                DetectScaler(split.Scaler).value\n",
    "                if (splitInfo.IsScaled and hasattr(split, 'Scaler') and split.Scaler is not None)\n",
    "                else \"None\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        ShowTitleBox(title, color=ConsoleColor.CYAN, boxLineStyle=TitleBoxLineStyle.SIMPLE)\n",
    "        ShowDfInfo(split.Train.X, f\"{title}\")\n",
    "        ShowDfHead(split.Train.X, f\"{title}\", headQty=5)\n",
    "\n",
    "        result1 = ApplyLogisticRegression(split, targetColumn=targetColumn)\n",
    "        result2 = ApplyKNN(split, targetColumn=targetColumn)\n",
    "\n",
    "        ShowTitleBox(\"RESULTADOS REGRESIÃ“N LOGÃSTICA\",\n",
    "                     color=ConsoleColor.YELLOW,\n",
    "                     boxLineStyle=TitleBoxLineStyle.HEAVY)\n",
    "\n",
    "        print(f\"Accuracy: {result1.Accuracy:.4f}\")\n",
    "        print(f\"Precision: {result1.Precision:.4f}\")\n",
    "        print(f\"Recall: {result1.Recall:.4f}\")\n",
    "        print(f\"F1 Score: {result1.F1:.4f}\")\n",
    "\n",
    "        print(\"Confusion Matrix:\")\n",
    "        PlotConfusionMatrix(\n",
    "            result1.ConfusionMatrix,\n",
    "            classNames=result1.Model.classes_,\n",
    "            title=\"Logistic Regression - Confusion Matrix\"\n",
    "        )\n",
    "\n",
    "        print(\"Classification Report:\")\n",
    "        print(result1.Report)\n",
    "\n",
    "        ShowTitleBox(\"RESULTADOS KNN\",\n",
    "                     color=ConsoleColor.YELLOW,\n",
    "                     boxLineStyle=TitleBoxLineStyle.HEAVY)\n",
    "\n",
    "        print(f\"Best K: {result2.K}\")\n",
    "        print(f\"Accuracy: {result2.Accuracy:.4f}\")\n",
    "        print(f\"Precision: {result2.Precision:.4f}\")\n",
    "        print(f\"Recall: {result2.Recall:.4f}\")\n",
    "        print(f\"F1 Score: {result2.F1:.4f}\")\n",
    "\n",
    "        print(\"Confusion Matrix:\")\n",
    "        PlotConfusionMatrix(\n",
    "            result2.ConfusionMatrix,\n",
    "            classNames= result2.Model.classes_,\n",
    "            title=\"KNN - Confusion Matrix\"\n",
    "        )\n",
    "\n",
    "        print(\"Classification Report:\")\n",
    "        print(result2.Report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ce2840",
   "metadata": {},
   "source": [
    "# TITANIC DATASET 1ï¸âƒ£ Business Understanding - Entender el negocio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16432a0",
   "metadata": {},
   "source": [
    "| Columna         | DescripciÃ³n                                                                                                                                                  |\n",
    "| --------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n",
    "| **PassengerId** | Identificador Ãºnico asignado a cada pasajero en el dataset. No tiene significado real en el viaje, solo sirve como Ã­ndice.                                   |\n",
    "| **Survived**    | Indica si el pasajero **sobreviviÃ³ (1)** o **muriÃ³ (0)**. Es la variable objetivo en la mayorÃ­a de proyectos de Machine Learning sobre este dataset.         |\n",
    "| **Pclass**      | Clase del boleto del pasajero: **1 = Primera clase**, **2 = Segunda clase**, **3 = Tercera clase**. Representa una medida indirecta de nivel socioeconÃ³mico. |\n",
    "| **Name**        | Nombre completo del pasajero, generalmente incluye tratamiento o tÃ­tulo social (Mr., Mrs., Miss, etc.).                                                      |\n",
    "| **Sex**         | Sexo del pasajero: **male** (hombre) o **female** (mujer).                                                                                                   |\n",
    "| **Age**         | Edad del pasajero en aÃ±os. Puede contener valores faltantes. Los niÃ±os pequeÃ±os pueden aparecer con valores decimales.                                       |\n",
    "| **SibSp**       | NÃºmero de **hermanos (siblings)** y **cÃ³nyuges (spouses)** que viajaban con el pasajero en el Titanic.                                                       |\n",
    "| **Parch**       | NÃºmero de **padres (parents)** e **hijos (children)** que viajaban con el pasajero.                                                                          |\n",
    "| **Ticket**      | CÃ³digo del boleto del pasajero. Puede incluir letras y nÃºmeros; no cumple un formato estÃ¡ndar.                                                               |\n",
    "| **Fare**        | Tarifa pagada por el pasajero por su boleto. El valor depende de la clase y otras condiciones del viaje.                                                     |\n",
    "| **Cabin**       | NÃºmero de cabina asignada. Muchos valores estÃ¡n vacÃ­os porque la mayorÃ­a de pasajeros no tenÃ­a cabina asignada (especialmente en 3ra clase).                 |\n",
    "| **Embarked**    | Puerto de embarque: **C = Cherbourg**, **Q = Queenstown**, **S = Southampton**.                                                                              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7516605",
   "metadata": {},
   "source": [
    "# TITANIC DATASET 2ï¸âƒ£ Data preparation - PreparaciÃ³n de los datos\n",
    "- Se descargan los archivos CSV que son Train, Test y los datos de Test de supervivencia. \n",
    "- Se eliminan columnas innecesarias. \"Cabin\", \"PassengerId\", \"Ticket\", \"Name\"\n",
    "- Se modifica las letras de embarque por los nombres de cada puerto real. \"S\" por \"Southampton\", \"C\" por \"Cherbourg\" y \"Q\" por \"Queenstown\".\n",
    "- Se aplica One Hot Encoding a la columna Embarked dado que es nominal y no tiene un orden especifico.\n",
    "- Se aplica One Hot Encoding a la columna Sex ya que es nominal y no depende de ningÃºn orden especÃ­fico.\n",
    "- Se creÃ³ una nueva columna para obtener el tamaÃ±o de la familia \"FamilySize\", es el total que incluye conyuges, hijos, padres y la persona.\n",
    "- Se imputÃ³ la columna Age por el promedio de los datos de Train.\n",
    "- Se imputÃ³ la columna Fare por el promedio de los datos de Train. Ya que en los datos de Test existÃ­a valores null.\n",
    "- Se elimina columnas no deseadas a propÃ³sito del usuario.\n",
    "- Finalmente se crean el DatasetSplit de datos que contiene los Datasets con la informaciÃ³n \"X\" y \"y\" tanto para Train como para Test.\n",
    "- Se muestra la tabla de correlaciÃ³n completa. Las tablas de correlaciÃ³n fuerte y dÃ©bil con un umbral de 0.4. Si las correlaciones son mayores o iguales al umbral se muestran en la tabla de correlaciÃ³n fuerte caso contrario no se muestran. Si las correlaciones son menores al umbral se muestran en la tabla de correlaciÃ³n dÃ©bil caso contrario no se muestran. \n",
    " \n",
    "Con un umbral de 0.4 existe correlaciÃ³n fuerte moderada del 26% en las columnas y el 74% no tiene correlaciÃ³n fuerte. \n",
    "\n",
    "- Finalmente quedan listos los datos listos para utilizar en un entrenamiento con algoritmo supervisado y posteriormente hacer los tests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e4b656",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOADS_DIR = \"Temp\"\n",
    "TITANIC_TEST_DATA_URL = \"https://github.com/UIDE-Tareas/6-Herramientas-Data-Science-Tarea1/raw/refs/heads/main/Datasets/TitanicTest.csv\"\n",
    "TITANIC_TEST_GENDER_SUB_DATA_URL = \"https://github.com/UIDE-Tareas/6-Herramientas-Data-Science-Tarea1/raw/refs/heads/main/Datasets/TitanicGenderSub.csv\"\n",
    "TITANIC_TRAIN_DATA_URL = \"https://github.com/UIDE-Tareas/6-Herramientas-Data-Science-Tarea1/raw/refs/heads/main/Datasets/TitanicTrain.csv\"\n",
    "TITANIC_TEST_FILENAME = os.path.join(DOWNLOADS_DIR, \"TitanicTest.csv\")\n",
    "TITANIC_TRAIN_FILENAME = os.path.join(DOWNLOADS_DIR, \"TitanicTrain.csv\")\n",
    "TITANIC_TEST_GENDER_SUB_FILENAME = os.path.join(DOWNLOADS_DIR, \"TitanicGenderSub.csv\")\n",
    "\n",
    "\n",
    "# Carga y preprocesa el dataset de Titanic, retornando el split Train/Test. \n",
    "def LoadTitanicDatasetSplit(toDrop: list[str]=[])-> DatasetSplit:\n",
    "    toDrop = [c for c in toDrop if c != \"Survived\"]\n",
    "    DownloadFile(TITANIC_TEST_DATA_URL, TITANIC_TEST_FILENAME, printInfo=False)\n",
    "    DownloadFile(TITANIC_TRAIN_DATA_URL, TITANIC_TRAIN_FILENAME, printInfo=False)\n",
    "    DownloadFile(TITANIC_TEST_GENDER_SUB_DATA_URL, TITANIC_TEST_GENDER_SUB_FILENAME, printInfo=False)\n",
    "    dfTrain = pd.read_csv(TITANIC_TRAIN_FILENAME)\n",
    "    dfTest = pd.read_csv(TITANIC_TEST_FILENAME)\n",
    "    dfTestGenderSub = pd.read_csv(TITANIC_TEST_GENDER_SUB_FILENAME)\n",
    "\n",
    "    toDiscard = [\"Cabin\", \"PassengerId\", \"Ticket\", \"Name\"]\n",
    "    dfTrain = dfTrain.drop(columns=toDiscard)\n",
    "    dfTest  = dfTest.drop(columns=toDiscard)\n",
    "\n",
    "    dtypeMap = {\n",
    "        \"Survived\": \"int64\",\n",
    "        \"Pclass\": \"int64\",\n",
    "        \"Sex\": \"string\",\n",
    "        \"Age\": \"float64\",\n",
    "        \"SibSp\": \"int64\",\n",
    "        \"Parch\": \"int64\",\n",
    "        \"Fare\": \"float64\",\n",
    "        \"Embarked\": \"string\"\n",
    "    }\n",
    "\n",
    "    dfTrain = dfTrain.astype(dtypeMap)\n",
    "    dfTest  = dfTest.astype({col: dtypeMap[col] for col in dfTest.columns})\n",
    "\n",
    "    mapEmbarked = {\n",
    "        \"S\": \"Southampton\",\n",
    "        \"C\": \"Cherbourg\",\n",
    "        \"Q\": \"Queenstown\"\n",
    "    }\n",
    "    dfTrain[\"Embarked\"] = dfTrain[\"Embarked\"].replace(mapEmbarked)\n",
    "    dfTest[\"Embarked\"]  = dfTest[\"Embarked\"].replace(mapEmbarked)\n",
    "\n",
    "    dfTrain = pd.get_dummies(dfTrain, columns=[\"Embarked\"], prefix=\"Embarked\")\n",
    "    dfTest  = pd.get_dummies(dfTest, columns=[\"Embarked\"], prefix=\"Embarked\")\n",
    "\n",
    "    dfTrain = pd.get_dummies(dfTrain, columns=[\"Sex\"], prefix=\"Sex\", drop_first=False)\n",
    "    dfTest  = pd.get_dummies(dfTest, columns=[\"Sex\"], prefix=\"Sex\", drop_first=False)\n",
    "\n",
    "    dfTrain[\"FamilySize\"] = dfTrain[\"SibSp\"] + dfTrain[\"Parch\"] + 1\n",
    "    dfTest[\"FamilySize\"] = dfTest[\"SibSp\"] + dfTest[\"Parch\"] + 1\n",
    "\n",
    "    meanAge = dfTrain[\"Age\"].mean()\n",
    "    dfTrain[\"Age\"] = dfTrain[\"Age\"].fillna(meanAge)\n",
    "    dfTest[\"Age\"]  = dfTest[\"Age\"].fillna(meanAge)\n",
    "\n",
    "    meanFare = dfTrain[\"Fare\"].mean()\n",
    "    dfTrain[\"Fare\"] = dfTrain[\"Fare\"].fillna(meanAge)\n",
    "    dfTest[\"Fare\"]  = dfTest[\"Fare\"].fillna(meanAge)\n",
    "\n",
    "    NormalizeColumnNames(dfTrain)\n",
    "    NormalizeColumnNames(dfTest)\n",
    "\n",
    "    dfTrain = DropColumns(dfTrain, toDrop, inplace=True)\n",
    "    dfTest  = DropColumns(dfTest, toDrop, inplace=True)\n",
    "\n",
    "    XTrain = dfTrain.drop(columns=[\"Survived\"])\n",
    "    yTrain = dfTrain[[\"Survived\"]].copy()\n",
    "    XTest  = dfTest\n",
    "    yTest  = dfTestGenderSub[[\"Survived\"]].copy()\n",
    "    return DatasetSplit(\n",
    "        Train=Dataset(X=XTrain, y=yTrain),\n",
    "        Test=Dataset(X=XTest, y=yTest)\n",
    "    )\n",
    "\n",
    "ShowTitleBox(\"EXPLORACIÃ“N INICIAL TITANIC DATASET\", color=ConsoleColor.MAGENTA, boxLineStyle= TitleBoxLineStyle.BLOCK)\n",
    "ShowTitleBox(\"CARGANDO EL DATASET\", color=ConsoleColor.CYAN, boxLineStyle= TitleBoxLineStyle.SIMPLE)\n",
    "split0 = LoadTitanicDatasetSplit()\n",
    "ShowTitleBox(\"INFO DEL DATASET\", color=ConsoleColor.CYAN, boxLineStyle= TitleBoxLineStyle.SIMPLE)\n",
    "title = \"Titanic Dataset Split\"\n",
    "ShowDatasetSplitInfo(split0, title, headQty=10)\n",
    "\n",
    "ShowTitleBox(\"MATRIZ DE CORRELACIÃ“N\", color=ConsoleColor.CYAN, boxLineStyle= TitleBoxLineStyle.SIMPLE)\n",
    "title = \"TITANIC Data original TRAIN\"\n",
    "CORR_UMBRAL = 0.4\n",
    "_=ShowDfCorrelation(split0.Train.X, title, level=CorrelationType.ALL, showTable=True, figsize=(10,8), annotate=True, umbral=CORR_UMBRAL)\n",
    "_=ShowDfCorrelation(split0.Train.X, title, level=CorrelationType.STRONG, showTable=False, figsize=(10,8), annotate=True, umbral=CORR_UMBRAL)\n",
    "_=ShowDfCorrelation(split0.Train.X, title, level=CorrelationType.WEAK, showTable=False, figsize=(10,8), annotate=True, umbral=CORR_UMBRAL)\n",
    "\n",
    "split1 = ScaleDatasetSplit( LoadTitanicDatasetSplit())\n",
    "split2 = ScaleDatasetSplit( LoadTitanicDatasetSplit(toDrop=[\"Sibsp\", \"Parch\"]), CreateScaler(ScalerType.STANDARD))\n",
    "split3 = ScaleDatasetSplit( LoadTitanicDatasetSplit(toDrop=[\"Fare\"]), StandardScaler())\n",
    "split4 = ScaleDatasetSplit(LoadTitanicDatasetSplit(toDrop=[\"Sibsp\", \"Parch\",\"Fare\"]), StandardScaler())\n",
    "\n",
    "split5 = ApplyPCA(ScaleDatasetSplit( LoadTitanicDatasetSplit()))\n",
    "split6 = ApplyPCA(ScaleDatasetSplit( LoadTitanicDatasetSplit(toDrop=[\"Sibsp\", \"Parch\"])))\n",
    "split7 = ApplyPCA(ScaleDatasetSplit( LoadTitanicDatasetSplit(toDrop=[\"Fare\"])))\n",
    "split8 = ApplyPCA(ScaleDatasetSplit(LoadTitanicDatasetSplit(toDrop=[\"Sibsp\", \"Parch\",\"Fare\"])))\n",
    "\n",
    "split9 = (LoadTitanicDatasetSplit())\n",
    "split10 = (LoadTitanicDatasetSplit(toDrop=[\"Sibsp\", \"Parch\"]))\n",
    "split11 = (LoadTitanicDatasetSplit(toDrop=[\"Fare\"]))\n",
    "split12 = (LoadTitanicDatasetSplit(toDrop=[\"Sibsp\", \"Parch\",\"Fare\"]))\n",
    "\n",
    "splitList = [split1, split2, split3, split4, split5, split6, split7, split8, split9, split10, split11, split12]\n",
    "\n",
    "EvaluateSplits(splitList, targetColumn=\"Survived\")\n",
    "ShowDfHead(GenerateMetricsTable(splitList, targetColumn=\"Survived\"), title=\"MÃ©tricas de todos los splits evaluados\", headQty=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094a7e66",
   "metadata": {},
   "source": [
    "# ðŸ Resultados ðŸ\n",
    "\n",
    "Se tiene 12 configuraciones distintas del dataset para probar:\n",
    "\n",
    "1. Scaled â€“ Standard â€“ Todas las caracterÃ­sticas\n",
    "2. Scaled â€“ Standard â€“ drop: SibSp, Parch\n",
    "3. Scaled â€“ Standard â€“ drop: Fare\n",
    "4. Scaled â€“ Standard â€“ drop: SibSp, Parch, Fare\n",
    "5. PCA â€“ Standard â€“ Todas las caracterÃ­sticas.\n",
    "6. PCA â€“ Standard â€“ drop: SibSp, Parch\n",
    "7. PCA â€“ Standard â€“ drop: Fare\n",
    "8. PCA â€“ Standard â€“ drop: SibSp, Parch, Fare\n",
    "9. Raw â€“ Todas las caracterÃ­sticas.\n",
    "10. Raw â€“ drop: SibSp, Parch\n",
    "11. Raw â€“ drop: Fare\n",
    "12. Raw â€“ drop: SibSp, Parch, Fare\n",
    "\n",
    "\n",
    "Y para cada uno, se calcula:\n",
    "\n",
    "- RegresiÃ³n LogÃ­stica\n",
    "- KNN\n",
    "\n",
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Index</th>\n",
    "      <th>Type</th>\n",
    "      <th>Scaler</th>\n",
    "      <th>LR_Accuracy</th>\n",
    "      <th>LR_Precision</th>\n",
    "      <th>LR_Recall</th>\n",
    "      <th>LR_F1</th>\n",
    "      <th>KNN_BestK</th>\n",
    "      <th>KNN_Accuracy</th>\n",
    "      <th>KNN_Precision</th>\n",
    "      <th>KNN_Recall</th>\n",
    "      <th>KNN_F1</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>1</td>\n",
    "      <td>SCALED</td>\n",
    "      <td>Standard</td>\n",
    "      <td>0.94</td>\n",
    "      <td>0.95</td>\n",
    "      <td>0.94</td>\n",
    "      <td>0.95</td>\n",
    "      <td>18</td>\n",
    "      <td>0.93</td>\n",
    "      <td>0.93</td>\n",
    "      <td>0.93</td>\n",
    "      <td>0.93</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>2</td>\n",
    "      <td>SCALED</td>\n",
    "      <td>Standard</td>\n",
    "      <td>0.94</td>\n",
    "      <td>0.94</td>\n",
    "      <td>0.94</td>\n",
    "      <td>0.94</td>\n",
    "      <td>23</td>\n",
    "      <td>0.93</td>\n",
    "      <td>0.93</td>\n",
    "      <td>0.93</td>\n",
    "      <td>0.92</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>3</td>\n",
    "      <td>SCALED</td>\n",
    "      <td>Standard</td>\n",
    "      <td>0.94</td>\n",
    "      <td>0.94</td>\n",
    "      <td>0.94</td>\n",
    "      <td>0.94</td>\n",
    "      <td>18</td>\n",
    "      <td>0.94</td>\n",
    "      <td>0.94</td>\n",
    "      <td>0.94</td>\n",
    "      <td>0.94</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>4</td>\n",
    "      <td>SCALED</td>\n",
    "      <td>Standard</td>\n",
    "      <td>0.94</td>\n",
    "      <td>0.94</td>\n",
    "      <td>0.94</td>\n",
    "      <td>0.94</td>\n",
    "      <td>29</td>\n",
    "      <td>0.93</td>\n",
    "      <td>0.94</td>\n",
    "      <td>0.93</td>\n",
    "      <td>0.93</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>5</td>\n",
    "      <td>PCA</td>\n",
    "      <td>None</td>\n",
    "      <td>0.95</td>\n",
    "      <td>0.95</td>\n",
    "      <td>0.95</td>\n",
    "      <td>0.95</td>\n",
    "      <td>29</td>\n",
    "      <td>0.95</td>\n",
    "      <td>0.95</td>\n",
    "      <td>0.95</td>\n",
    "      <td>0.95</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>5</th>\n",
    "      <td>6</td>\n",
    "      <td>PCA</td>\n",
    "      <td>None</td>\n",
    "      <td>0.95</td>\n",
    "      <td>0.95</td>\n",
    "      <td>0.95</td>\n",
    "      <td>0.95</td>\n",
    "      <td>30</td>\n",
    "      <td>0.94</td>\n",
    "      <td>0.94</td>\n",
    "      <td>0.94</td>\n",
    "      <td>0.94</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>6</th>\n",
    "      <td>7</td>\n",
    "      <td>PCA</td>\n",
    "      <td>None</td>\n",
    "      <td>0.94</td>\n",
    "      <td>0.94</td>\n",
    "      <td>0.94</td>\n",
    "      <td>0.94</td>\n",
    "      <td>19</td>\n",
    "      <td>0.94</td>\n",
    "      <td>0.94</td>\n",
    "      <td>0.94</td>\n",
    "      <td>0.94</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>7</th>\n",
    "      <td>8</td>\n",
    "      <td>PCA</td>\n",
    "      <td>None</td>\n",
    "      <td>0.94</td>\n",
    "      <td>0.94</td>\n",
    "      <td>0.94</td>\n",
    "      <td>0.94</td>\n",
    "      <td>15</td>\n",
    "      <td>0.94</td>\n",
    "      <td>0.94</td>\n",
    "      <td>0.94</td>\n",
    "      <td>0.93</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>8</th>\n",
    "      <td>9</td>\n",
    "      <td>RAW</td>\n",
    "      <td>None</td>\n",
    "      <td>0.95</td>\n",
    "      <td>0.95</td>\n",
    "      <td>0.95</td>\n",
    "      <td>0.95</td>\n",
    "      <td>7</td>\n",
    "      <td>0.70</td>\n",
    "      <td>0.70</td>\n",
    "      <td>0.70</td>\n",
    "      <td>0.70</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>9</th>\n",
    "      <td>10</td>\n",
    "      <td>RAW</td>\n",
    "      <td>None</td>\n",
    "      <td>0.94</td>\n",
    "      <td>0.94</td>\n",
    "      <td>0.94</td>\n",
    "      <td>0.94</td>\n",
    "      <td>8</td>\n",
    "      <td>0.71</td>\n",
    "      <td>0.70</td>\n",
    "      <td>0.71</td>\n",
    "      <td>0.70</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>10</th>\n",
    "      <td>11</td>\n",
    "      <td>RAW</td>\n",
    "      <td>None</td>\n",
    "      <td>0.94</td>\n",
    "      <td>0.94</td>\n",
    "      <td>0.94</td>\n",
    "      <td>0.94</td>\n",
    "      <td>8</td>\n",
    "      <td>0.82</td>\n",
    "      <td>0.82</td>\n",
    "      <td>0.82</td>\n",
    "      <td>0.81</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>11</th>\n",
    "      <td>12</td>\n",
    "      <td>RAW</td>\n",
    "      <td>None</td>\n",
    "      <td>0.94</td>\n",
    "      <td>0.94</td>\n",
    "      <td>0.94</td>\n",
    "      <td>0.94</td>\n",
    "      <td>9</td>\n",
    "      <td>0.82</td>\n",
    "      <td>0.82</td>\n",
    "      <td>0.82</td>\n",
    "      <td>0.82</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "Este dataset no se ve muy afectado por la eliminaciÃ³n o adiciÃ³n de nuevas features.\n",
    "\n",
    "En la mayorÃ­a de los casos da similar rendimiento en los scores. 0.93 - 0.95.\n",
    "\n",
    "Para Logistic Regression la mejor combinaciÃ³n es cualquiera porque no se ve afectado el modelo por los datos raw, escalados o aplicado PCA. El modelo KNN si se ve afectado por los datos sin escalar por lo que es recomendable escalar primero los datos o aplicar PCA. \n",
    "\n",
    "Al aplicar PCA o escalado ante de entrenar con KNN, el valor de K en KNN aumenta.\n",
    "\n",
    "Para este dataset queda recomendado utilizar LogisticRegression en cualquier condiciÃ³n.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
